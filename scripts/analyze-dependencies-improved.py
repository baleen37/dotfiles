#!/usr/bin/env python3
"""
ê°œì„ ëœ Nix ì˜ì¡´ì„± ë¶„ì„ ë„êµ¬
ë” ì •í™•í•œ import êµ¬ë¬¸ íŒŒì‹±ê³¼ ì˜ì¡´ì„± ì¶”ì  ê¸°ëŠ¥ ì œê³µ
"""

import os
import re
import json
import sys
from pathlib import Path
from typing import Dict, List, Set, Tuple
from collections import defaultdict, deque

class ImprovedNixDependencyAnalyzer:
    def __init__(self, repo_path: str):
        self.repo_path = Path(repo_path)
        self.dependencies = defaultdict(set)
        self.reverse_dependencies = defaultdict(set)
        self.file_contents = {}
        self.nix_files = []

    def scan_repository(self):
        """ì €ì¥ì†Œì˜ ëª¨ë“  .nix íŒŒì¼ì„ ìŠ¤ìº”í•©ë‹ˆë‹¤."""
        print("ğŸ” Scanning repository for .nix files...")

        for file_path in self.repo_path.rglob("*.nix"):
            if file_path.is_file():
                relative_path = file_path.relative_to(self.repo_path)
                self.nix_files.append(relative_path)

                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        self.file_contents[str(relative_path)] = content
                except Exception as e:
                    print(f"âŒ Error reading {relative_path}: {e}")

        print(f"âœ… Found {len(self.nix_files)} .nix files")
        return self.nix_files

    def normalize_path(self, import_path: str, current_file: str) -> str:
        """import ê²½ë¡œë¥¼ ì •ê·œí™”í•©ë‹ˆë‹¤."""
        # ./ë¡œ ì‹œì‘í•˜ëŠ” ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬
        if import_path.startswith('./'):
            import_path = import_path[2:]
            current_dir = Path(current_file).parent
            normalized = (current_dir / import_path).as_posix()
            return normalized

        # ../ë¡œ ì‹œì‘í•˜ëŠ” ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬
        elif import_path.startswith('../'):
            current_dir = Path(current_file).parent
            try:
                resolved = (current_dir / import_path).resolve()
                relative_to_repo = resolved.relative_to(self.repo_path)
                return str(relative_to_repo)
            except:
                return import_path

        # ì ˆëŒ€ ê²½ë¡œë‚˜ ë‹¨ìˆœ íŒŒì¼ëª…ì€ ê·¸ëŒ€ë¡œ ë°˜í™˜
        return import_path

    def extract_imports(self, content: str, current_file: str) -> Set[str]:
        """íŒŒì¼ì—ì„œ ëª¨ë“  import êµ¬ë¬¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        imports = set()

        # ë‹¤ì–‘í•œ import íŒ¨í„´ ì •ì˜
        patterns = [
            # import ./path/file.nix
            r'import\s+(\./[^;\s}]+\.nix)',
            # import ../path/file.nix
            r'import\s+(\.\.\/[^;\s}]+\.nix)',
            # import /absolute/path.nix
            r'import\s+(\/[^;\s}]+\.nix)',
            # import file.nix (ê°™ì€ ë””ë ‰í† ë¦¬)
            r'import\s+([^/\s;{}]+\.nix)(?!\s*\{)',
            # ../modules/something.nix (ì§ì ‘ ê²½ë¡œ ì°¸ì¡°)
            r'(?<!")(\.\./[^"\s;{}]+\.nix)(?!")',
            # ./something.nix (ì§ì ‘ ê²½ë¡œ ì°¸ì¡°)
            r'(?<!")(\./[^"\s;{}]+\.nix)(?!")',
        ]

        for pattern in patterns:
            matches = re.findall(pattern, content, re.MULTILINE)
            for match in matches:
                if isinstance(match, tuple):
                    match = match[0]

                # ê²½ë¡œ ì •ê·œí™”
                normalized = self.normalize_path(match, current_file)

                # ì‹¤ì œ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                if normalized in [str(f) for f in self.nix_files]:
                    imports.add(normalized)
                elif (self.repo_path / normalized).exists():
                    imports.add(normalized)

        return imports

    def analyze_all_dependencies(self):
        """ëª¨ë“  íŒŒì¼ì˜ ì˜ì¡´ì„±ì„ ë¶„ì„í•©ë‹ˆë‹¤."""
        print("\nğŸ”— Analyzing dependencies for all files...")

        for nix_file in self.nix_files:
            file_str = str(nix_file)
            content = self.file_contents[file_str]

            # íŒŒì¼ì˜ ëª¨ë“  import ì¶”ì¶œ
            imports = self.extract_imports(content, file_str)

            # ì˜ì¡´ì„± ê·¸ë˜í”„ êµ¬ì„±
            self.dependencies[file_str] = imports

            # ì—­ ì˜ì¡´ì„± ê·¸ë˜í”„ êµ¬ì„±
            for imported_file in imports:
                self.reverse_dependencies[imported_file].add(file_str)

        total_deps = sum(len(deps) for deps in self.dependencies.values())
        print(f"  ğŸ“Š Total dependencies found: {total_deps}")

        return self.dependencies

    def find_entry_points(self) -> Set[str]:
        """ì§„ì…ì  íŒŒì¼ë“¤ì„ ì‹ë³„í•©ë‹ˆë‹¤."""
        entry_points = set()

        # ëª…ì‹œì  ì§„ì…ì ë“¤
        explicit_entries = [
            'flake.nix',
            'default.nix'
        ]

        for entry in explicit_entries:
            if entry in [str(f) for f in self.nix_files]:
                entry_points.add(entry)

        # hosts/ ë””ë ‰í† ë¦¬ì˜ default.nixë“¤ë„ ì§„ì…ì 
        for nix_file in self.nix_files:
            if str(nix_file).startswith('hosts/') and nix_file.name == 'default.nix':
                entry_points.add(str(nix_file))

        # app ê´€ë ¨ íŒŒì¼ë“¤ë„ ì§„ì…ì ìœ¼ë¡œ ê°„ì£¼
        for nix_file in self.nix_files:
            if '/build' in str(nix_file) or '/apply' in str(nix_file):
                continue  # ì‹¤í–‰ íŒŒì¼ì€ ì œì™¸
            if str(nix_file).startswith('apps/'):
                entry_points.add(str(nix_file))

        return entry_points

    def find_reachable_files(self, entry_points: Set[str]) -> Set[str]:
        """ì§„ì…ì ì—ì„œ ë„ë‹¬ ê°€ëŠ¥í•œ ëª¨ë“  íŒŒì¼ì„ ì°¾ìŠµë‹ˆë‹¤."""
        reachable = set()
        queue = deque(entry_points)

        while queue:
            current = queue.popleft()
            if current in reachable:
                continue

            reachable.add(current)

            # í˜„ì¬ íŒŒì¼ì´ ì˜ì¡´í•˜ëŠ” íŒŒì¼ë“¤ì„ íì— ì¶”ê°€
            for dependency in self.dependencies.get(current, set()):
                if dependency not in reachable:
                    queue.append(dependency)

        return reachable

    def find_unused_files(self):
        """ì‹¤ì œë¡œ ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” íŒŒì¼ë“¤ì„ ì°¾ìŠµë‹ˆë‹¤."""
        print("\nğŸ—‘ï¸  Finding truly unused files...")

        # ì§„ì…ì  ì‹ë³„
        entry_points = self.find_entry_points()
        print(f"  ğŸšª Entry points found: {len(entry_points)}")
        for ep in sorted(entry_points):
            print(f"     - {ep}")

        # ì§„ì…ì ì—ì„œ ë„ë‹¬ ê°€ëŠ¥í•œ íŒŒì¼ë“¤ ì°¾ê¸°
        reachable = self.find_reachable_files(entry_points)
        print(f"  ğŸ”— Reachable files: {len(reachable)}")

        # ëª¨ë“  íŒŒì¼ ì§‘í•©
        all_files = set(str(f) for f in self.nix_files)

        # ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” íŒŒì¼ë“¤
        unused = all_files - reachable

        print(f"  ğŸ“Š Total files: {len(all_files)}")
        print(f"  âœ… Used files: {len(reachable)}")
        print(f"  âŒ Unused files: {len(unused)}")

        return {
            'unused': sorted(unused),
            'used': sorted(reachable),
            'entry_points': sorted(entry_points)
        }

    def analyze_dependency_depth(self):
        """ì˜ì¡´ì„± ê¹Šì´ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
        print("\nğŸ“ Analyzing dependency depth...")

        entry_points = self.find_entry_points()
        depth_map = {}

        for entry in entry_points:
            depths = {}
            queue = deque([(entry, 0)])
            visited = set()

            while queue:
                current, depth = queue.popleft()
                if current in visited:
                    continue
                visited.add(current)

                if current not in depths or depth < depths[current]:
                    depths[current] = depth

                for dependency in self.dependencies.get(current, set()):
                    if dependency not in visited:
                        queue.append((dependency, depth + 1))

            for file, depth in depths.items():
                if file not in depth_map or depth < depth_map[file]:
                    depth_map[file] = depth

        return depth_map

    def generate_detailed_report(self):
        """ìƒì„¸í•œ ë¶„ì„ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        print("\nğŸ“ Generating detailed analysis report...")

        # ì˜ì¡´ì„± ë¶„ì„ ì‹¤í–‰
        self.analyze_all_dependencies()

        # ë¯¸ì‚¬ìš© íŒŒì¼ ë¶„ì„
        unused_analysis = self.find_unused_files()

        # ì˜ì¡´ì„± ê¹Šì´ ë¶„ì„
        depth_map = self.analyze_dependency_depth()

        # í†µê³„ ê³„ì‚°
        stats = {
            'total_files': len(self.nix_files),
            'lib_files': len([f for f in self.nix_files if str(f).startswith('lib/')]),
            'module_files': len([f for f in self.nix_files if str(f).startswith('modules/')]),
            'test_files': len([f for f in self.nix_files if str(f).startswith('tests/')]),
            'script_files': len([f for f in self.nix_files if str(f).startswith('scripts/')]),
            'host_files': len([f for f in self.nix_files if str(f).startswith('hosts/')]),
            'total_dependencies': sum(len(deps) for deps in self.dependencies.values()),
            'unused_files_count': len(unused_analysis['unused']),
            'used_files_count': len(unused_analysis['used']),
            'entry_points_count': len(unused_analysis['entry_points']),
            'max_dependency_depth': max(depth_map.values()) if depth_map else 0
        }

        # ì¹´í…Œê³ ë¦¬ë³„ ë¯¸ì‚¬ìš© íŒŒì¼ ë¶„ì„
        unused_by_category = self._categorize_unused_files(unused_analysis['unused'])

        # ì˜ì¡´ì„± ìˆœí™˜ ê²€ì‚¬
        cycles = self._detect_dependency_cycles()

        report = {
            'timestamp': str(Path().resolve()),
            'statistics': stats,
            'unused_analysis': unused_analysis,
            'unused_by_category': unused_by_category,
            'dependency_cycles': cycles,
            'depth_analysis': {
                'max_depth': stats['max_dependency_depth'],
                'depth_distribution': self._calculate_depth_distribution(depth_map)
            },
            'recommendations': self._generate_improved_recommendations(unused_analysis, stats, cycles)
        }

        return report

    def _categorize_unused_files(self, unused_files: List[str]) -> Dict[str, List[str]]:
        """ë¯¸ì‚¬ìš© íŒŒì¼ë“¤ì„ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤."""
        categories = {
            'lib': [],
            'modules': [],
            'tests': [],
            'hosts': [],
            'overlays': [],
            'scripts': [],
            'other': []
        }

        for file in unused_files:
            if file.startswith('lib/'):
                categories['lib'].append(file)
            elif file.startswith('modules/'):
                categories['modules'].append(file)
            elif file.startswith('tests/'):
                categories['tests'].append(file)
            elif file.startswith('hosts/'):
                categories['hosts'].append(file)
            elif file.startswith('overlays/'):
                categories['overlays'].append(file)
            elif file.startswith('scripts/') and file.endswith('.nix'):
                categories['scripts'].append(file)
            else:
                categories['other'].append(file)

        return categories

    def _detect_dependency_cycles(self) -> List[List[str]]:
        """ì˜ì¡´ì„± ìˆœí™˜ì„ ê²€ì‚¬í•©ë‹ˆë‹¤."""
        def dfs(node, visited, rec_stack, path):
            visited.add(node)
            rec_stack.add(node)
            path.append(node)

            for neighbor in self.dependencies.get(node, set()):
                if neighbor not in visited:
                    cycle = dfs(neighbor, visited, rec_stack, path[:])
                    if cycle:
                        return cycle
                elif neighbor in rec_stack:
                    # ìˆœí™˜ ë°œê²¬
                    cycle_start = path.index(neighbor)
                    return path[cycle_start:] + [neighbor]

            rec_stack.remove(node)
            return None

        visited = set()
        cycles = []

        for node in self.dependencies:
            if node not in visited:
                cycle = dfs(node, visited, set(), [])
                if cycle:
                    cycles.append(cycle)

        return cycles

    def _calculate_depth_distribution(self, depth_map: Dict[str, int]) -> Dict[int, int]:
        """ê¹Šì´ë³„ íŒŒì¼ ë¶„í¬ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        distribution = defaultdict(int)
        for depth in depth_map.values():
            distribution[depth] += 1
        return dict(distribution)

    def _generate_improved_recommendations(self, unused_analysis: Dict, stats: Dict, cycles: List) -> List[str]:
        """ê°œì„ ëœ ê¶Œì¥ì‚¬í•­ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        recommendations = []

        unused_count = len(unused_analysis['unused'])
        if unused_count > 0:
            recommendations.append(f"ğŸ—‘ï¸ Review {unused_count} unused files for potential removal")

        if len(cycles) > 0:
            recommendations.append(f"ğŸ”„ Fix {len(cycles)} dependency cycles detected")

        if stats['max_dependency_depth'] > 5:
            recommendations.append(f"ğŸ“ Consider flattening dependency tree (max depth: {stats['max_dependency_depth']})")

        lib_files = stats['lib_files']
        if lib_files > 30:
            recommendations.append(f"ğŸ“š Large lib/ directory ({lib_files} files) - consider modularization")

        dependency_ratio = stats['total_dependencies'] / stats['total_files'] if stats['total_files'] > 0 else 0
        if dependency_ratio > 1.5:
            recommendations.append(f"ğŸ”— High dependency ratio ({dependency_ratio:.2f}) - review coupling")

        return recommendations

def main():
    if len(sys.argv) > 1:
        repo_path = sys.argv[1]
    else:
        repo_path = os.getcwd()

    print(f"ğŸš€ Starting improved dependency analysis for: {repo_path}")
    print("=" * 70)

    analyzer = ImprovedNixDependencyAnalyzer(repo_path)

    # ì €ì¥ì†Œ ìŠ¤ìº”
    analyzer.scan_repository()

    # ë¶„ì„ ì‹¤í–‰ ë° ë³´ê³ ì„œ ìƒì„±
    report = analyzer.generate_detailed_report()

    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 70)
    print("ğŸ“Š IMPROVED ANALYSIS SUMMARY")
    print("=" * 70)

    stats = report['statistics']
    print(f"ğŸ“ Total .nix files: {stats['total_files']}")
    print(f"ğŸ“š Library files: {stats['lib_files']}")
    print(f"ğŸ§© Module files: {stats['module_files']}")
    print(f"ğŸ§ª Test files: {stats['test_files']}")
    print(f"ğŸ  Host files: {stats['host_files']}")
    print(f"ğŸ”— Total dependencies: {stats['total_dependencies']}")
    print(f"ğŸšª Entry points: {stats['entry_points_count']}")
    print(f"âœ… Used files: {stats['used_files_count']}")
    print(f"âŒ Unused files: {stats['unused_files_count']}")
    print(f"ğŸ“ Max dependency depth: {stats['max_dependency_depth']}")

    # ì¹´í…Œê³ ë¦¬ë³„ ë¯¸ì‚¬ìš© íŒŒì¼
    print("\nğŸ—‚ï¸ UNUSED FILES BY CATEGORY:")
    for category, files in report['unused_by_category'].items():
        if files:
            print(f"   ğŸ“ {category}: {len(files)} files")

    # ì˜ì¡´ì„± ìˆœí™˜
    if report['dependency_cycles']:
        print(f"\nğŸ”„ DEPENDENCY CYCLES DETECTED: {len(report['dependency_cycles'])}")

    print("\nğŸ’¡ RECOMMENDATIONS:")
    for i, rec in enumerate(report['recommendations'], 1):
        print(f"   {i}. {rec}")

    # ìƒì„¸ ë³´ê³ ì„œ ì €ì¥
    output_file = Path(repo_path) / "improved-dependency-analysis.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ“‹ Detailed report saved to: {output_file}")
    print("=" * 70)
    print("âœ… Improved analysis completed successfully!")

if __name__ == "__main__":
    main()
